strict digraph {
"audio_to_text.py___speech_collate_fn";
list;
zip;
len;
item;
stack;
tensor;
ValueError;
append;
max;
pad;
"audio_to_text.py__expand_sharded_filepaths";
isinstance;
braceexpand;
datastore_path_to_webdataset_url;
info;
replace;
is_datastore_path;
is_tarred_path;
warning;
"audio_to_text.py__cache_datastore_manifests";
sum;
split;
min;
is_available;
is_initialized;
debug;
barrier;
is_global_rank_zero;
is_datastore_cache_shared;
int;
cache_data;
RuntimeError;
cpu_count;
get;
str;
dirname;
DataStoreObject;
open;
tqdm;
all;
loads;
join;
Pool;
imap;
"audio_to_text.py__shard_manifests_if_needed";
expand_sharded_filepaths;
"audio_to_text.py____init__";
ASRAudioText;
"audio_to_text.py__process_text_by_id";
process_text_by_sample;
"audio_to_text.py__process_text_by_file_id";
"audio_to_text.py__process_text_by_sample";
"audio_to_text.py__output_types";
NeuralType;
AudioSignal;
tuple;
LengthsType;
LabelsType;
cache_datastore_manifests;
ASRManifestProcessor;
WaveformFeaturizer;
type;
"audio_to_text.py____getitem__";
_process_sample;
"audio_to_text.py___process_sample";
process;
long;
"audio_to_text.py____len__";
"audio_to_text.py___collate_fn";
_speech_collate_fn;
make_parser;
__init__;
super;
hasattr;
text_to_ids;
TokenizerWrapper;
extend;
shard_manifests_if_needed;
_compute_len;
DataPipeline;
SimpleShardList;
shuffle;
tarfile_to_samples;
rename;
to_tuple;
map;
"audio_to_text.py___filter";
TarredAudioFilter;
next;
splitext;
basename;
"audio_to_text.py___loop_offsets";
TarredAudioLoopOffsets;
"audio_to_text.py___build_sample";
BytesIO;
close;
"audio_to_text.py____iter__";
__iter__;
"audio_to_text.py___compute_len";
cuda;
all_reduce;
BucketingIterator;
ceil;
float;
iter;
"audio_to_text.py____next__";
range;
RandomState;
permutation;
enumerate;
"audio_to_text.py__cache_data";
"audio_to_text.py____call__";
"ctc.py__input_types";
LogprobsType;
"ctc.py__output_types";
LossType;
"ctc.py____init__";
"ctc.py__reduce";
mean;
"ctc.py__forward";
typecheck;
transpose;
forward;
reduce;
"wer.py__move_dimension_to_the_front";
permute;
"wer.py__word_error_rate";
eval;
format;
"wer.py__word_error_rate_detail";
cer;
compute_measures;
"wer.py__word_error_rate_per_utt";
"wer.py____init__";
add_state;
rnnt_decoder_predictions_tensor;
ctc_decoder_predictions_tensor;
TypeError;
decode_predictions_tensor;
"wer.py__update";
no_grad;
cpu;
decode;
move_dimension_to_the_front;
tolist;
decode_tokens_to_str;
numpy;
"wer.py__compute";
detach;
"ctc_models.py____init__";
from_config_dict;
CTCLoss;
CTCDecoding;
WER;
setup_optimization_flags;
setup_interctc;
setup_adapters;
open_dict;
structured;
to_container;
"ctc_models.py__transcribe";
transcribe;
change_decoding_strategy;
"ctc_models.py__change_vocabulary";
to_config_dict;
deepcopy;
create;
merge;
"ctc_models.py__change_decoding_strategy";
to_yaml;
"ctc_models.py___setup_dataloader_from_config";
inject_dataloader_value_from_model_config;
get_audio_to_text_char_dataset_from_config;
DataLoader;
get_lhotse_dataloader_from_config;
get_semi_sorted_batch_sampler;
LhotseSpeechToTextBpeDataset;
"ctc_models.py__setup_training_data";
_update_dataset_config;
_setup_dataloader_from_config;
"ctc_models.py__setup_validation_data";
"ctc_models.py__setup_test_data";
"ctc_models.py__input_types";
SpectrogramType;
"ctc_models.py__output_types";
"ctc_models.py__forward";
encoder;
decoder;
argmax;
preprocessor;
spec_augmentation;
"ctc_models.py__training_step";
is_access_enabled;
is_interctc_enabled;
loss;
add_auxiliary_losses;
add_interctc_losses;
update;
reset_registry;
set_access_enabled;
compute;
reset;
"ctc_models.py__predict_step";
"ctc_models.py__validation_pass";
log;
"ctc_models.py__validation_step";
validation_pass;
"ctc_models.py__multi_validation_epoch_end";
multi_validation_epoch_end;
finalize_interctc_metrics;
"ctc_models.py__multi_test_epoch_end";
multi_test_epoch_end;
"ctc_models.py__test_step";
items;
"ctc_models.py___transcribe_forward";
dict;
"ctc_models.py___transcribe_output_processing";
pop;
process_timestamp_outputs;
empty;
copy_;
clone;
device;
"ctc_models.py___setup_transcribe_dataloader";
DictConfig;
"ctc_models.py__list_available_models";
PretrainedModelInfo;
"audio_to_text.py___speech_collate_fn" -> list;
"audio_to_text.py___speech_collate_fn" -> zip;
"audio_to_text.py___speech_collate_fn" -> len;
"audio_to_text.py___speech_collate_fn" -> item;
"audio_to_text.py___speech_collate_fn" -> stack;
"audio_to_text.py___speech_collate_fn" -> tensor;
"audio_to_text.py___speech_collate_fn" -> ValueError;
"audio_to_text.py___speech_collate_fn" -> append;
"audio_to_text.py___speech_collate_fn" -> max;
"audio_to_text.py___speech_collate_fn" -> pad;
"audio_to_text.py__expand_sharded_filepaths" -> isinstance;
"audio_to_text.py__expand_sharded_filepaths" -> ValueError;
"audio_to_text.py__expand_sharded_filepaths" -> list;
"audio_to_text.py__expand_sharded_filepaths" -> braceexpand;
"audio_to_text.py__expand_sharded_filepaths" -> datastore_path_to_webdataset_url;
"audio_to_text.py__expand_sharded_filepaths" -> info;
"audio_to_text.py__expand_sharded_filepaths" -> replace;
"audio_to_text.py__expand_sharded_filepaths" -> is_datastore_path;
"audio_to_text.py__expand_sharded_filepaths" -> is_tarred_path;
"audio_to_text.py__expand_sharded_filepaths" -> warning;
"audio_to_text.py__expand_sharded_filepaths" -> len;
"audio_to_text.py__cache_datastore_manifests" -> isinstance;
"audio_to_text.py__cache_datastore_manifests" -> sum;
"audio_to_text.py__cache_datastore_manifests" -> split;
"audio_to_text.py__cache_datastore_manifests" -> is_datastore_path;
"audio_to_text.py__cache_datastore_manifests" -> min;
"audio_to_text.py__cache_datastore_manifests" -> is_available;
"audio_to_text.py__cache_datastore_manifests" -> is_initialized;
"audio_to_text.py__cache_datastore_manifests" -> debug;
"audio_to_text.py__cache_datastore_manifests" -> barrier;
"audio_to_text.py__cache_datastore_manifests" -> is_global_rank_zero;
"audio_to_text.py__cache_datastore_manifests" -> is_datastore_cache_shared;
"audio_to_text.py__cache_datastore_manifests" -> int;
"audio_to_text.py__cache_datastore_manifests" -> info;
"audio_to_text.py__cache_datastore_manifests" -> cache_data;
"audio_to_text.py__cache_datastore_manifests" -> warning;
"audio_to_text.py__cache_datastore_manifests" -> RuntimeError;
"audio_to_text.py__cache_datastore_manifests" -> cpu_count;
"audio_to_text.py__cache_datastore_manifests" -> get;
"audio_to_text.py__cache_datastore_manifests" -> str;
"audio_to_text.py__cache_datastore_manifests" -> dirname;
"audio_to_text.py__cache_datastore_manifests" -> DataStoreObject;
"audio_to_text.py__cache_datastore_manifests" -> open;
"audio_to_text.py__cache_datastore_manifests" -> tqdm;
"audio_to_text.py__cache_datastore_manifests" -> all;
"audio_to_text.py__cache_datastore_manifests" -> loads;
"audio_to_text.py__cache_datastore_manifests" -> join;
"audio_to_text.py__cache_datastore_manifests" -> append;
"audio_to_text.py__cache_datastore_manifests" -> Pool;
"audio_to_text.py__cache_datastore_manifests" -> list;
"audio_to_text.py__cache_datastore_manifests" -> imap;
"audio_to_text.py__cache_datastore_manifests" -> len;
"audio_to_text.py__shard_manifests_if_needed" -> expand_sharded_filepaths;
"audio_to_text.py__shard_manifests_if_needed" -> is_available;
"audio_to_text.py__shard_manifests_if_needed" -> warning;
"audio_to_text.py__shard_manifests_if_needed" -> is_initialized;
"audio_to_text.py____init__" -> ASRAudioText;
"audio_to_text.py____init__" -> cache_datastore_manifests;
"audio_to_text.py____init__" -> ASRManifestProcessor;
"audio_to_text.py____init__" -> WaveformFeaturizer;
"audio_to_text.py____init__" -> type;
"audio_to_text.py____init__" -> split;
"audio_to_text.py____init__" -> make_parser;
"audio_to_text.py____init__" -> __init__;
"audio_to_text.py____init__" -> super;
"audio_to_text.py____init__" -> hasattr;
"audio_to_text.py____init__" -> isinstance;
"audio_to_text.py____init__" -> text_to_ids;
"audio_to_text.py____init__" -> TokenizerWrapper;
"audio_to_text.py____init__" -> extend;
"audio_to_text.py____init__" -> shard_manifests_if_needed;
"audio_to_text.py____init__" -> _compute_len;
"audio_to_text.py____init__" -> expand_sharded_filepaths;
"audio_to_text.py____init__" -> DataPipeline;
"audio_to_text.py____init__" -> SimpleShardList;
"audio_to_text.py____init__" -> shuffle;
"audio_to_text.py____init__" -> tarfile_to_samples;
"audio_to_text.py____init__" -> rename;
"audio_to_text.py____init__" -> to_tuple;
"audio_to_text.py____init__" -> map;
"audio_to_text.py____init__" -> RandomState;
"audio_to_text.py____init__" -> list;
"audio_to_text.py__process_text_by_id" -> process_text_by_sample;
"audio_to_text.py__process_text_by_file_id" -> process_text_by_sample;
"audio_to_text.py__process_text_by_sample" -> len;
"audio_to_text.py__output_types" -> NeuralType;
"audio_to_text.py__output_types" -> AudioSignal;
"audio_to_text.py__output_types" -> tuple;
"audio_to_text.py__output_types" -> LengthsType;
"audio_to_text.py__output_types" -> LabelsType;
"audio_to_text.py____getitem__" -> isinstance;
"audio_to_text.py____getitem__" -> _process_sample;
"audio_to_text.py___process_sample" -> process;
"audio_to_text.py___process_sample" -> process_text_by_sample;
"audio_to_text.py___process_sample" -> long;
"audio_to_text.py___process_sample" -> tensor;
"audio_to_text.py____len__" -> len;
"audio_to_text.py____len__" -> int;
"audio_to_text.py____len__" -> ceil;
"audio_to_text.py____len__" -> float;
"audio_to_text.py___collate_fn" -> _speech_collate_fn;
"audio_to_text.py___filter" -> TarredAudioFilter;
"audio_to_text.py___filter" -> next;
"audio_to_text.py___filter" -> splitext;
"audio_to_text.py___filter" -> basename;
"audio_to_text.py___loop_offsets" -> TarredAudioLoopOffsets;
"audio_to_text.py___loop_offsets" -> next;
"audio_to_text.py___loop_offsets" -> len;
"audio_to_text.py___build_sample" -> splitext;
"audio_to_text.py___build_sample" -> BytesIO;
"audio_to_text.py___build_sample" -> process;
"audio_to_text.py___build_sample" -> close;
"audio_to_text.py___build_sample" -> process_text_by_sample;
"audio_to_text.py___build_sample" -> basename;
"audio_to_text.py___build_sample" -> long;
"audio_to_text.py___build_sample" -> len;
"audio_to_text.py___build_sample" -> tensor;
"audio_to_text.py____iter__" -> __iter__;
"audio_to_text.py____iter__" -> BucketingIterator;
"audio_to_text.py____iter__" -> iter;
"audio_to_text.py____iter__" -> permutation;
"audio_to_text.py____iter__" -> len;
"audio_to_text.py____iter__" -> isinstance;
"audio_to_text.py____iter__" -> enumerate;
"audio_to_text.py___compute_len" -> is_available;
"audio_to_text.py___compute_len" -> is_initialized;
"audio_to_text.py___compute_len" -> cuda;
"audio_to_text.py___compute_len" -> all_reduce;
"audio_to_text.py___compute_len" -> int;
"audio_to_text.py___compute_len" -> info;
"audio_to_text.py___compute_len" -> len;
"audio_to_text.py___compute_len" -> tensor;
"audio_to_text.py____next__" -> range;
"audio_to_text.py____next__" -> append;
"audio_to_text.py____next__" -> len;
"audio_to_text.py____next__" -> next;
"audio_to_text.py____next__" -> splitext;
"audio_to_text.py____next__" -> basename;
"audio_to_text.py__cache_data" -> min;
"audio_to_text.py__cache_data" -> is_datastore_path;
"audio_to_text.py__cache_data" -> cpu_count;
"audio_to_text.py__cache_data" -> info;
"audio_to_text.py__cache_data" -> get;
"audio_to_text.py__cache_data" -> debug;
"audio_to_text.py__cache_data" -> str;
"audio_to_text.py__cache_data" -> dirname;
"audio_to_text.py__cache_data" -> DataStoreObject;
"audio_to_text.py__cache_data" -> open;
"audio_to_text.py__cache_data" -> tqdm;
"audio_to_text.py__cache_data" -> all;
"audio_to_text.py__cache_data" -> RuntimeError;
"audio_to_text.py__cache_data" -> loads;
"audio_to_text.py__cache_data" -> join;
"audio_to_text.py__cache_data" -> append;
"audio_to_text.py__cache_data" -> Pool;
"audio_to_text.py__cache_data" -> list;
"audio_to_text.py__cache_data" -> imap;
"audio_to_text.py__cache_data" -> len;
"audio_to_text.py____call__" -> text_to_ids;
"audio_to_text.py____call__" -> isinstance;
"audio_to_text.py____call__" -> extend;
"ctc.py__input_types" -> NeuralType;
"ctc.py__input_types" -> LogprobsType;
"ctc.py__input_types" -> LabelsType;
"ctc.py__input_types" -> tuple;
"ctc.py__input_types" -> LengthsType;
"ctc.py__output_types" -> NeuralType;
"ctc.py__output_types" -> LossType;
"ctc.py____init__" -> __init__;
"ctc.py____init__" -> ValueError;
"ctc.py____init__" -> super;
"ctc.py__reduce" -> mean;
"ctc.py__reduce" -> sum;
"ctc.py__forward" -> typecheck;
"ctc.py__forward" -> long;
"ctc.py__forward" -> transpose;
"ctc.py__forward" -> forward;
"ctc.py__forward" -> reduce;
"ctc.py__forward" -> super;
"wer.py__move_dimension_to_the_front" -> list;
"wer.py__move_dimension_to_the_front" -> permute;
"wer.py__move_dimension_to_the_front" -> range;
"wer.py__word_error_rate" -> zip;
"wer.py__word_error_rate" -> len;
"wer.py__word_error_rate" -> ValueError;
"wer.py__word_error_rate" -> eval;
"wer.py__word_error_rate" -> float;
"wer.py__word_error_rate" -> format;
"wer.py__word_error_rate" -> list;
"wer.py__word_error_rate" -> split;
"wer.py__word_error_rate_detail" -> zip;
"wer.py__word_error_rate_detail" -> len;
"wer.py__word_error_rate_detail" -> ValueError;
"wer.py__word_error_rate_detail" -> format;
"wer.py__word_error_rate_detail" -> list;
"wer.py__word_error_rate_detail" -> split;
"wer.py__word_error_rate_detail" -> float;
"wer.py__word_error_rate_detail" -> cer;
"wer.py__word_error_rate_detail" -> compute_measures;
"wer.py__word_error_rate_per_utt" -> zip;
"wer.py__word_error_rate_per_utt" -> len;
"wer.py__word_error_rate_per_utt" -> ValueError;
"wer.py__word_error_rate_per_utt" -> float;
"wer.py__word_error_rate_per_utt" -> format;
"wer.py__word_error_rate_per_utt" -> list;
"wer.py__word_error_rate_per_utt" -> split;
"wer.py__word_error_rate_per_utt" -> append;
"wer.py__word_error_rate_per_utt" -> cer;
"wer.py__word_error_rate_per_utt" -> compute_measures;
"wer.py____init__" -> __init__;
"wer.py____init__" -> isinstance;
"wer.py____init__" -> add_state;
"wer.py____init__" -> super;
"wer.py____init__" -> rnnt_decoder_predictions_tensor;
"wer.py____init__" -> tensor;
"wer.py____init__" -> ctc_decoder_predictions_tensor;
"wer.py____init__" -> TypeError;
"wer.py____init__" -> decode_predictions_tensor;
"wer.py____init__" -> type;
"wer.py__update" -> zip;
"wer.py__update" -> tensor;
"wer.py__update" -> no_grad;
"wer.py__update" -> cpu;
"wer.py__update" -> range;
"wer.py__update" -> decode;
"wer.py__update" -> info;
"wer.py__update" -> isinstance;
"wer.py__update" -> len;
"wer.py__update" -> eval;
"wer.py__update" -> move_dimension_to_the_front;
"wer.py__update" -> item;
"wer.py__update" -> tolist;
"wer.py__update" -> decode_tokens_to_str;
"wer.py__update" -> append;
"wer.py__update" -> list;
"wer.py__update" -> split;
"wer.py__update" -> long;
"wer.py__update" -> numpy;
"wer.py__compute" -> float;
"wer.py__compute" -> detach;
"ctc_models.py____init__" -> __init__;
"ctc_models.py____init__" -> from_config_dict;
"ctc_models.py____init__" -> CTCLoss;
"ctc_models.py____init__" -> get;
"ctc_models.py____init__" -> CTCDecoding;
"ctc_models.py____init__" -> WER;
"ctc_models.py____init__" -> setup_optimization_flags;
"ctc_models.py____init__" -> setup_interctc;
"ctc_models.py____init__" -> setup_adapters;
"ctc_models.py____init__" -> open_dict;
"ctc_models.py____init__" -> hasattr;
"ctc_models.py____init__" -> structured;
"ctc_models.py____init__" -> super;
"ctc_models.py____init__" -> ValueError;
"ctc_models.py____init__" -> info;
"ctc_models.py____init__" -> len;
"ctc_models.py____init__" -> to_container;
"ctc_models.py____init__" -> format;
"ctc_models.py__transcribe" -> transcribe;
"ctc_models.py__transcribe" -> info;
"ctc_models.py__transcribe" -> change_decoding_strategy;
"ctc_models.py__transcribe" -> super;
"ctc_models.py__transcribe" -> open_dict;
"ctc_models.py__transcribe" -> get;
"ctc_models.py__change_vocabulary" -> warning;
"ctc_models.py__change_vocabulary" -> to_config_dict;
"ctc_models.py__change_vocabulary" -> deepcopy;
"ctc_models.py__change_vocabulary" -> len;
"ctc_models.py__change_vocabulary" -> from_config_dict;
"ctc_models.py__change_vocabulary" -> CTCLoss;
"ctc_models.py__change_vocabulary" -> structured;
"ctc_models.py__change_vocabulary" -> create;
"ctc_models.py__change_vocabulary" -> merge;
"ctc_models.py__change_vocabulary" -> CTCDecoding;
"ctc_models.py__change_vocabulary" -> WER;
"ctc_models.py__change_vocabulary" -> info;
"ctc_models.py__change_vocabulary" -> ValueError;
"ctc_models.py__change_vocabulary" -> to_container;
"ctc_models.py__change_vocabulary" -> open_dict;
"ctc_models.py__change_vocabulary" -> get;
"ctc_models.py__change_decoding_strategy" -> structured;
"ctc_models.py__change_decoding_strategy" -> create;
"ctc_models.py__change_decoding_strategy" -> merge;
"ctc_models.py__change_decoding_strategy" -> CTCDecoding;
"ctc_models.py__change_decoding_strategy" -> WER;
"ctc_models.py__change_decoding_strategy" -> get;
"ctc_models.py__change_decoding_strategy" -> info;
"ctc_models.py__change_decoding_strategy" -> to_container;
"ctc_models.py__change_decoding_strategy" -> open_dict;
"ctc_models.py__change_decoding_strategy" -> to_yaml;
"ctc_models.py___setup_dataloader_from_config" -> inject_dataloader_value_from_model_config;
"ctc_models.py___setup_dataloader_from_config" -> get;
"ctc_models.py___setup_dataloader_from_config" -> get_audio_to_text_char_dataset_from_config;
"ctc_models.py___setup_dataloader_from_config" -> isinstance;
"ctc_models.py___setup_dataloader_from_config" -> hasattr;
"ctc_models.py___setup_dataloader_from_config" -> DataLoader;
"ctc_models.py___setup_dataloader_from_config" -> get_lhotse_dataloader_from_config;
"ctc_models.py___setup_dataloader_from_config" -> get_semi_sorted_batch_sampler;
"ctc_models.py___setup_dataloader_from_config" -> RuntimeError;
"ctc_models.py___setup_dataloader_from_config" -> LhotseSpeechToTextBpeDataset;
"ctc_models.py___setup_dataloader_from_config" -> make_parser;
"ctc_models.py___setup_dataloader_from_config" -> type;
"ctc_models.py__setup_training_data" -> _update_dataset_config;
"ctc_models.py__setup_training_data" -> _setup_dataloader_from_config;
"ctc_models.py__setup_training_data" -> hasattr;
"ctc_models.py__setup_training_data" -> isinstance;
"ctc_models.py__setup_training_data" -> int;
"ctc_models.py__setup_training_data" -> warning;
"ctc_models.py__setup_training_data" -> ceil;
"ctc_models.py__setup_training_data" -> len;
"ctc_models.py__setup_validation_data" -> _update_dataset_config;
"ctc_models.py__setup_validation_data" -> _setup_dataloader_from_config;
"ctc_models.py__setup_test_data" -> _update_dataset_config;
"ctc_models.py__setup_test_data" -> _setup_dataloader_from_config;
"ctc_models.py__input_types" -> hasattr;
"ctc_models.py__input_types" -> AudioSignal;
"ctc_models.py__input_types" -> NeuralType;
"ctc_models.py__input_types" -> tuple;
"ctc_models.py__input_types" -> LengthsType;
"ctc_models.py__input_types" -> SpectrogramType;
"ctc_models.py__output_types" -> NeuralType;
"ctc_models.py__output_types" -> LogprobsType;
"ctc_models.py__output_types" -> tuple;
"ctc_models.py__output_types" -> LengthsType;
"ctc_models.py__output_types" -> LabelsType;
"ctc_models.py__forward" -> typecheck;
"ctc_models.py__forward" -> encoder;
"ctc_models.py__forward" -> decoder;
"ctc_models.py__forward" -> argmax;
"ctc_models.py__forward" -> ValueError;
"ctc_models.py__forward" -> preprocessor;
"ctc_models.py__forward" -> spec_augmentation;
"ctc_models.py__training_step" -> is_access_enabled;
"ctc_models.py__training_step" -> is_interctc_enabled;
"ctc_models.py__training_step" -> loss;
"ctc_models.py__training_step" -> add_auxiliary_losses;
"ctc_models.py__training_step" -> add_interctc_losses;
"ctc_models.py__training_step" -> update;
"ctc_models.py__training_step" -> reset_registry;
"ctc_models.py__training_step" -> set_access_enabled;
"ctc_models.py__training_step" -> isinstance;
"ctc_models.py__training_step" -> forward;
"ctc_models.py__training_step" -> hasattr;
"ctc_models.py__training_step" -> compute;
"ctc_models.py__training_step" -> reset;
"ctc_models.py__training_step" -> tensor;
"ctc_models.py__predict_step" -> ctc_decoder_predictions_tensor;
"ctc_models.py__predict_step" -> isinstance;
"ctc_models.py__predict_step" -> list;
"ctc_models.py__predict_step" -> forward;
"ctc_models.py__predict_step" -> numpy;
"ctc_models.py__predict_step" -> zip;
"ctc_models.py__predict_step" -> detach;
"ctc_models.py__predict_step" -> cpu;
"ctc_models.py__validation_pass" -> is_interctc_enabled;
"ctc_models.py__validation_pass" -> loss;
"ctc_models.py__validation_pass" -> add_interctc_losses;
"ctc_models.py__validation_pass" -> update;
"ctc_models.py__validation_pass" -> compute;
"ctc_models.py__validation_pass" -> reset;
"ctc_models.py__validation_pass" -> log;
"ctc_models.py__validation_pass" -> is_access_enabled;
"ctc_models.py__validation_pass" -> set_access_enabled;
"ctc_models.py__validation_pass" -> isinstance;
"ctc_models.py__validation_pass" -> forward;
"ctc_models.py__validation_pass" -> tensor;
"ctc_models.py__validation_pass" -> reset_registry;
"ctc_models.py__validation_step" -> validation_pass;
"ctc_models.py__validation_step" -> append;
"ctc_models.py__validation_step" -> type;
"ctc_models.py__validation_step" -> len;
"ctc_models.py__multi_validation_epoch_end" -> multi_validation_epoch_end;
"ctc_models.py__multi_validation_epoch_end" -> finalize_interctc_metrics;
"ctc_models.py__multi_validation_epoch_end" -> super;
"ctc_models.py__multi_test_epoch_end" -> multi_test_epoch_end;
"ctc_models.py__multi_test_epoch_end" -> finalize_interctc_metrics;
"ctc_models.py__multi_test_epoch_end" -> super;
"ctc_models.py__test_step" -> validation_pass;
"ctc_models.py__test_step" -> replace;
"ctc_models.py__test_step" -> append;
"ctc_models.py__test_step" -> items;
"ctc_models.py__test_step" -> type;
"ctc_models.py__test_step" -> len;
"ctc_models.py___transcribe_forward" -> forward;
"ctc_models.py___transcribe_forward" -> dict;
"ctc_models.py___transcribe_output_processing" -> pop;
"ctc_models.py___transcribe_output_processing" -> ctc_decoder_predictions_tensor;
"ctc_models.py___transcribe_output_processing" -> cpu;
"ctc_models.py___transcribe_output_processing" -> range;
"ctc_models.py___transcribe_output_processing" -> process_timestamp_outputs;
"ctc_models.py___transcribe_output_processing" -> empty;
"ctc_models.py___transcribe_output_processing" -> copy_;
"ctc_models.py___transcribe_output_processing" -> clone;
"ctc_models.py___transcribe_output_processing" -> device;
"ctc_models.py___setup_transcribe_dataloader" -> get;
"ctc_models.py___setup_transcribe_dataloader" -> _setup_dataloader_from_config;
"ctc_models.py___setup_transcribe_dataloader" -> join;
"ctc_models.py___setup_transcribe_dataloader" -> min;
"ctc_models.py___setup_transcribe_dataloader" -> to_container;
"ctc_models.py___setup_transcribe_dataloader" -> len;
"ctc_models.py___setup_transcribe_dataloader" -> DictConfig;
"ctc_models.py___setup_transcribe_dataloader" -> cpu_count;
"ctc_models.py__list_available_models" -> PretrainedModelInfo;
"ctc_models.py__list_available_models" -> append;
}
